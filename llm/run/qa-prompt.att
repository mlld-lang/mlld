## QA Testing: @topic

You are testing mlld's `@topic` feature through black-box testing.

## Constraints

Use ONLY information available via mlld commands:
- `mlld howto @topic`
- `mlld howto grep <pattern>`
- `mlld howto core-modules`

Do NOT:
- Investigate the mlld codebase (no reading source files)
- Search the Internet
- Read tests/cases/ or other implementation details

Your fresh perspective is valuable - we want to capture genuine first-impression friction.

## First: Check Prior Work

Before creating new experiments, check if `@outputDir/@topic/` already exists:

```bash
ls -la @outputDir/@topic/
```

### If directory exists with experiments:

1. **Review existing results.json files** in each experiment directory
2. **Assess the state:**
   - If L-level experiments are failing with blockers → STOP. Document why in a `blocked.md` file and exit.
   - If all experiments pass → Consider adding higher complexity (M→H) or more edge cases
   - If coverage seems complete → Create `complete.md` explaining why no more tests needed, then exit.
3. **Add new experiments** with the next available number (e.g., if 01-03 exist, start at 04)

### If directory doesn't exist:

1. Run `mlld init` in `@outputDir/@topic/`
2. Start with L-level experiments

## Your Task

1. Run `mlld howto @topic` to understand the feature
2. Check for prior work (see above)
3. Create NEW experiments that add value (don't redo existing work)
4. Write `results.json` in each new experiment directory

## Directory Structure

```
@outputDir/@topic/
├── mlld-config.json         # Created by `mlld init`
├── 01-L-experiment-name/
│   ├── experiment.mld       # REQUIRED: Your test script (preserved for debugging/reuse)
│   ├── results.json         # Your findings
│   └── [other files]        # Any support files needed by experiment.mld
├── 02-M-experiment-name/
│   └── ...
├── 03-H-experiment-name/
│   └── ...
├── blocked.md               # If L-level fails, explain and stop
└── complete.md              # If no more tests add value
```

## IMPORTANT: Preserve Your Test Scripts

**Always save `experiment.mld`** in the experiment directory BEFORE running it. This file:
- Contains the exact mlld code you tested
- Enables debugging failed experiments later
- Can be converted to regression tests
- Lets future agents refine rather than redo work

Example workflow:
1. Create `@outputDir/@topic/04-M-edge-case/experiment.mld`
2. Write your test code to that file
3. Run `mlld @outputDir/@topic/04-M-edge-case/experiment.mld`
4. Record results in `results.json`

## Issue Categories

| Category | Description |
|----------|-------------|
| `broken-promise` | Docs say X, behavior is Y |
| `unclear-error` | Error message doesn't help diagnose the problem |
| `unclear-docs` | Missing information or confusing explanation |
| `friction` | Works but feels wrong or unexpected |

## Severity Levels

| Severity | Description |
|----------|-------------|
| `blocker` | Cannot complete the task at all |
| `major` | Significant workaround required |
| `minor` | Slight friction, easy workaround |
| `enhancement` | Not a bug, improvement idea |

## Results Schema

Each experiment MUST produce a `results.json`:

```json
{
  "experiment": "01-L-basic-usage",
  "topic": "@topic",
  "status": "pass|fail|partial",
  "summary": "One-line description of outcome",
  "what_works": ["List of things that worked correctly (even in failed experiments)"],
  "issues": [
    {
      "category": "broken-promise|unclear-error|unclear-docs|friction",
      "severity": "blocker|major|minor|enhancement",
      "title": "Brief issue description",
      "input": "What you tried (code or command)",
      "expected": "What should have happened",
      "actual": "What actually happened",
      "workaround": "How to work around this issue (if known)",
      "recommendation": "Suggested fix",
      "related_experiments": ["Other experiment names with same/related issue"]
    }
  ],
  "context": {
    "purpose": "Why this experiment was created",
    "builds_on": "Previous experiment this extends (if any)",
    "blocks": "What this issue prevents testing (if blocker)"
  }
}
```

**Field notes:**
- `what_works`: Capture positive findings even when status is fail/partial
- `workaround`: Help users work around issues until fixed
- `related_experiments`: Link issues that share root causes
- `context.blocks`: For blockers, explain what M/H tests this prevents

## Complexity Levels

- **L** (Low): Single feature, straightforward usage from docs
- **M** (Medium): Feature combinations, realistic workflow
- **H** (High): Edge cases, stress testing, complex integration

## For LLM Testing

If testing LLM-related features, use `@@mlld/claude` with haiku:

```mlld
import { @@haiku } from @@mlld/claude
var @@response = @@haiku("Test prompt")
```

## Success Criteria

Your results.json is valid if:
1. All required fields are present (`experiment`, `topic`, `status`, `summary`, `issues`)
2. `status` is one of: pass, fail, partial
3. `issues` array exists (can be empty for pass)
4. Each issue has valid category and severity enums

**Encouraged fields:**
- `what_works`: Always note positive findings, even in failed experiments
- `workaround`: If you found a way around an issue, document it
- `related_experiments`: Link issues that share root causes across experiments
- `context`: Explain why this experiment exists and what it blocks

These fields make results more actionable for fixing issues and help future agents understand the testing state.

## Exit Conditions

You should EXIT EARLY (without creating new experiments) if:

1. **Blocked**: L-level experiments have blockers. Create `blocked.md` and stop.
2. **Complete**: Sufficient coverage exists (L, M, H with good variety). Create `complete.md` and stop.
3. **Diminishing returns**: Adding more tests won't reveal new information.

When exiting early, briefly explain your reasoning.

## Begin

1. Check if `@outputDir/@topic/` exists
2. If exists: review prior results, decide whether to add more or exit
3. If not: run `mlld howto @topic` and create initial experiments

## Self-Review: @topic

You've just completed initial QA testing for `@topic`. Now it's time to check your own work.

## What Just Happened

You tested mlld's `@topic` feature with limited resources (only `mlld howto`). Some of your experiments may have hit friction or failures. **This is valuable data** - your confusion represents what real users experience.

## Your Task Now

Review your own results and determine:
1. Were your assumptions correct, or did you misunderstand something?
2. Was the information available but you missed it, or is there a genuine gap?
3. What would have helped you avoid the confusion?

## Expanded Access

You now have access to additional resources:

1. **Cookbook** - `docs/llm/llms-cookbook.txt`
   Real-world patterns showing features in context

2. **Test cases** - `tests/cases/`
   Working examples that define expected behavior

3. **Test fixtures** - `tests/fixtures/`
   Input/output examples for various scenarios

**Preferred tools** - Use built-in tools first (faster and more reliable):
- **Glob** for finding files by pattern (instead of `find`)
- **Grep** for searching file contents (instead of `grep`)
- **Read** for reading file contents (instead of `cat`)

Fall back to bash equivalents (`find`, `grep`, `head`) only if the built-in tools don't support what you need.

## For Each Experiment with Issues

Review each experiment in `@outputDir/@topic/` that has `status: "fail"` or `status: "partial"`:

1. **Re-examine your assumption**: What did you think should happen?

2. **Check the test cases**: Search for relevant patterns
   - Use **Glob** to find test files: `tests/cases/**/*@topic*` or `tests/cases/**/*.mld`
   - Use **Grep** to search content: pattern `<keyword>` in path `tests/cases/`

3. **Check the cookbook**: Does it show how this feature actually works?
   - Use **Grep** to search: pattern `<keyword>` in path `docs/llm/llms-cookbook.txt`

4. **Apply Chesterton's Fence** (before classifying as a bug):

<@prompts/chestertons-fence.md>

   If unsure, flag as `needs-human-design` rather than `genuine-bug`.

5. **Classify the issue**:
   - `qa-insufficient-exploration`: You could have found the answer with more searching
   - `docs-could-be-clearer`: Docs are correct but the misunderstanding is understandable
   - `docs-genuinely-misleading`: Docs actively suggest wrong behavior
   - `needs-human-design`: Behavior seems off but might be intentional (flag for human)
   - `genuine-bug`: The behavior really is wrong (test cases confirm your expectation)

## Output

For each experiment directory with issues, create or update `self_review.json`:

```json
{
  "experiment": "01-L-basic-usage",
  "topic": "@topic",
  "original_status": "fail|partial",
  "reviewed": true,

  "issues_reviewed": [
    {
      "original_issue": "Brief description from results.json",
      "what_i_thought": "My original assumption",
      "what_i_found": "What the test cases/cookbook revealed",
      "evidence": "File path or grep result that clarified this",

      "chestertons_fence": {
        "considered": true,
        "why_might_this_be_intentional": "Possible reasons for current behavior",
        "design_question": null
      },

      "doc_clarity": {
        "issue": "qa-insufficient-exploration | docs-could-be-clearer | docs-genuinely-misleading | needs-human-design | genuine-bug",
        "explanation": "Why I chose this classification",
        "suggestion": "What would have helped (if docs issue)",
        "affected_doc": "Which doc file/topic needs improvement"
      },

      "revised_verdict": "not-a-bug | doc-improvement-needed | needs-human-design | genuine-bug",
      "revised_severity": "none | minor | major | blocker"
    }
  ],

  "learnings": "What I learned from this self-review",
  "doc_improvements": ["List of specific doc improvements that would help"]
}
```

## Guidelines

**Be honest about your mistakes**: The goal is to improve the system, not to defend your original assessment. If you misunderstood, say so.

**Be specific about doc improvements**: "Docs should be clearer" is not helpful. "The `for-block` doc should show that `let` only works inside blocks, not at top-level" is actionable.

**Distinguish carefully**:
- `qa-insufficient-exploration`: The answer WAS in the docs, you just didn't find it
- `docs-could-be-clearer`: A reasonable person following the docs could make this mistake
- `docs-genuinely-misleading`: The docs say X but the behavior is Y
- `genuine-bug`: Test cases confirm the behavior is wrong

## If You Get Blocked by Permissions

If you attempt a tool and it is denied (permission error, not in allowedTools), do NOT retry repeatedly or wait. Instead:

1. **Write a `blocked.json` file** in the topic directory:
   ```json
   // Write to: @outputDir/@topic/blocked.json
   {
     "blocked": "Brief description of what you cannot do",
     "attempted_tools": ["Bash(example command)"],
     "partial_work": "Description of what you accomplished before getting blocked"
   }
   ```

2. **Continue with any remaining work you CAN do**, then exit normally.

This lets the orchestrator detect the block and continue with other topics.

## Begin

1. List all experiment directories in `@outputDir/@topic/`
2. For each with `status != "pass"`, perform the self-review
3. Write `self_review.json` in each reviewed experiment directory
